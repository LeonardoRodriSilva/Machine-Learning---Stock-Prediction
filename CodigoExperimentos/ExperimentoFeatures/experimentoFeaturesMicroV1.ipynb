{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 11s]\n",
      "val_loss: 0.0015139520401135087\n",
      "\n",
      "Best val_loss So Far: 0.0012230363208800554\n",
      "Total elapsed time: 00h 01m 32s\n",
      "\n",
      "Melhores Hiperparâmetros encontrados:\n",
      "{'num_lstm_layers': 1, 'units_lstm_0': 100, 'dropout_lstm_0': 0.1, 'num_dense_layers': 1, 'learning_rate': 0.001, 'units_lstm_1': 100, 'dropout_lstm_1': 0.2, 'units_dense_0': 64, 'dropout_dense_0': 0.1}\n",
      "\n",
      "Iniciando TimeSeriesSplit CV (5 folds) para avaliar os melhores HPs...\n",
      "   Fold 1/5...\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "     Fold 1 MAE: 0.3093, RMSE: 0.5263, R2: 0.8584\n",
      "   Fold 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\envs\\ambiente\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "     Fold 2 MAE: 1.6835, RMSE: 1.8177, R2: 0.2603\n",
      "   Fold 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\envs\\ambiente\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "     Fold 3 MAE: 0.4456, RMSE: 0.5670, R2: 0.6917\n",
      "   Fold 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\envs\\ambiente\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "     Fold 4 MAE: 1.9427, RMSE: 2.0324, R2: -1.4906\n",
      "   Fold 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\envs\\ambiente\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "     Fold 5 MAE: 0.4171, RMSE: 0.5610, R2: 0.9468\n",
      "\n",
      "Resultados Médios da Validação Cruzada (TimeSeriesSplit):\n",
      "   Avg MAE: 0.9596, Avg MSE: 1.6696, Avg RMSE: 1.1009, Avg R2: 0.2533\n",
      "\n",
      "Treinando modelo final com melhores HPs...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\envs\\ambiente\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.1030\n",
      "Epoch 2/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0106\n",
      "Epoch 3/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0079\n",
      "Epoch 4/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0058\n",
      "Epoch 5/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0057\n",
      "Epoch 6/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0053\n",
      "Epoch 7/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0045\n",
      "Epoch 8/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049\n",
      "Epoch 9/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048\n",
      "Epoch 10/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041\n",
      "Epoch 11/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040\n",
      "Epoch 12/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043\n",
      "Epoch 13/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040\n",
      "Epoch 14/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039\n",
      "Epoch 15/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0042\n",
      "Epoch 16/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 17/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0035\n",
      "Epoch 18/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0039\n",
      "Epoch 19/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032\n",
      "Epoch 20/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0032\n",
      "Epoch 21/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0038\n",
      "Epoch 22/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030\n",
      "Epoch 23/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0033\n",
      "Epoch 24/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028\n",
      "Epoch 26/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0031\n",
      "Epoch 27/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031\n",
      "Epoch 28/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027\n",
      "Epoch 29/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030\n",
      "Epoch 30/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028\n",
      "Epoch 31/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028\n",
      "Epoch 32/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028\n",
      "Epoch 33/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025\n",
      "Epoch 34/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 35/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036\n",
      "Epoch 36/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
      "Epoch 37/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023\n",
      "Epoch 38/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 39/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025\n",
      "Epoch 40/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0025\n",
      "Epoch 41/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 42/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 43/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026\n",
      "Epoch 44/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026\n",
      "Epoch 45/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 46/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 47/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
      "Epoch 48/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 49/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 50/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
      "Epoch 51/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0024\n",
      "Epoch 52/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 53/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
      "Epoch 55/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019\n",
      "Epoch 56/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021\n",
      "Epoch 57/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020\n",
      "Epoch 59/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 60/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021\n",
      "Epoch 62/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0017\n",
      "Epoch 63/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
      "Epoch 64/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 65/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 66/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 67/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 68/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 69/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 70/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018 \n",
      "Epoch 71/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 72/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 73/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 74/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 75/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016\n",
      "Epoch 76/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 77/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016\n",
      "Epoch 78/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 79/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016\n",
      "Epoch 80/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0015\n",
      "Epoch 81/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016\n",
      "Epoch 82/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0015   \n",
      "Epoch 84/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
      "Epoch 85/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016\n",
      "Epoch 86/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015\n",
      "Epoch 88/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017    \n",
      "Epoch 89/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
      "Epoch 90/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 91/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
      "Epoch 92/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015\n",
      "Epoch 93/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015\n",
      "Epoch 94/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015\n",
      "Epoch 95/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014\n",
      "Epoch 96/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014\n",
      "Epoch 97/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
      "Epoch 98/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0014    \n",
      "Epoch 99/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 100/100\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0014   \n",
      "Restoring model weights from the end of the best epoch: 98.\n",
      "\n",
      "Avaliando modelo final no conjunto de teste (2023)...\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "Resultados da Avaliação Final (Teste 2023):\n",
      "   MAE Teste: 0.6475, MSE Teste: 0.5426, RMSE Teste: 0.7366, R² Teste: 0.7686\n",
      "\n",
      "Salvando resultados...\n",
      "Métricas salvas em resultados_lstm_cv_micro_tratados/GGBR3.SA_Janela_4_metrics.csv\n",
      "Hiperparâmetros salvos em resultados_lstm_cv_micro_tratados/GGBR3.SA_Janela_4_hiperparametros.csv\n",
      "Previsões do teste salvas em resultados_lstm_cv_micro_tratados/GGBR3.SA_Janela_4_previsoes_teste_final.csv\n",
      "Gráfico do teste salvo em resultados_lstm_cv_micro_tratados/GGBR3.SA_Janela_4_grafico_teste_final.png\n",
      "--- Experimento Concluído: GGBR3.SA, Janela 4 ---\n",
      "\n",
      "--- TODOS OS EXPERIMENTOS COM DADOS MICRO TRATADOS CONCLUÍDOS ---\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- Função para CARREGAR dados MICRO de um ticker ---\n",
    "def carregar_dados_ticker_micro(ticker, pasta_dados_tratados):\n",
    "    \"\"\"\n",
    "    Carrega os dados microeconômicos pré-processados de um arquivo CSV específico.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): O nome do ticker (ex: \"PETR4.SA\").\n",
    "        pasta_dados_tratados (str): O caminho para a pasta contendo os CSVs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (pd.DataFrame contendo os dados carregados,\n",
    "                list de nomes das colunas de features,\n",
    "                str com o nome da coluna target)\n",
    "               Retorna (None, None, None) se o arquivo não for encontrado ou houver erro.\n",
    "    \"\"\"\n",
    "    nome_arquivo = f\"{ticker}_dados_unificados.csv\"\n",
    "    caminho_arquivo = os.path.join(pasta_dados_tratados, nome_arquivo)\n",
    "\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        print(f\"Erro: Arquivo não encontrado para {ticker} em {caminho_arquivo}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        print(f\"Carregando dados micro tratados de: {caminho_arquivo}\")\n",
    "        # A primeira coluna sem nome contém as datas, então tornamos ela o índice\n",
    "        df = pd.read_csv(caminho_arquivo, index_col=0, parse_dates=True)\n",
    "\n",
    "        # Verificar se as colunas necessárias existem\n",
    "        colunas_esperadas = ['Preço', 'ROA', 'ROE', 'Margem Líquida', 'P/L', 'VP', 'Preço_anterior']\n",
    "        colunas_faltantes = [col for col in colunas_esperadas if col not in df.columns]\n",
    "        if colunas_faltantes:\n",
    "            print(f\"Aviso: Colunas faltantes no DataFrame para {ticker}: {colunas_faltantes}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Criar colunas de feature e target\n",
    "        # Similar ao código macro, precisamos definir o target como o preço atual\n",
    "        # e as features como todas as outras colunas\n",
    "        \n",
    "        # Renomear 'Preço' para 'Close_Target' (mantendo consistência com o código macro)\n",
    "        df['Close_Target'] = df['Preço']\n",
    "        \n",
    "        # Usar 'Preço_anterior' como 'Close_Feature' (preço em t-1)\n",
    "        df['Close_Feature'] = df['Preço_anterior']\n",
    "        \n",
    "        # Definir target e features\n",
    "        target_col = 'Close_Target'\n",
    "        \n",
    "        # Features são todas as colunas exceto o target ('Preço'/'Close_Target')\n",
    "        feature_cols = [col for col in df.columns if col != 'Preço' and col != 'Close_Target']\n",
    "        \n",
    "        print(f\"Colunas de Features identificadas para {ticker}: {feature_cols}\")\n",
    "        print(f\"Coluna Target identificada para {ticker}: {target_col}\")\n",
    "\n",
    "        # Remover linhas onde o target é NaN\n",
    "        df.dropna(subset=[target_col], inplace=True)\n",
    "\n",
    "        return df, feature_cols, target_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar ou processar o arquivo {caminho_arquivo}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# --- Função para criar janelas MULTIVARIADAS (sem alterações) ---\n",
    "def criar_janelas_multivariadas(features_array, target_array, janela):\n",
    "    X, y = [], []\n",
    "    if len(features_array) <= janela:\n",
    "        print(f\"Aviso em criar_janelas: comprimento dos dados ({len(features_array)}) não é maior que a janela ({janela}). Retornando vazio.\")\n",
    "        return np.array(X), np.array(y)\n",
    "    for i in range(len(features_array) - janela):\n",
    "        window_features = features_array[i:(i + janela), :]\n",
    "        X.append(window_features)\n",
    "        # Target correspondente: valor na linha i+janela no target_array\n",
    "        # (que já representa T+1 em relação às features originais)\n",
    "        y.append(target_array[i + janela])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# --- Função build_model (sem alterações) ---\n",
    "def build_model(hp, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 2)):\n",
    "        return_sequences = i < hp.Int('num_lstm_layers', 1, 2) - 1\n",
    "        model.add(layers.LSTM(\n",
    "            units=hp.Int(f'units_lstm_{i}', min_value=50, max_value=150, step=50),\n",
    "            return_sequences=return_sequences,\n",
    "            input_shape=input_shape if i == 0 else None\n",
    "        ))\n",
    "        model.add(layers.Dropout(hp.Float(f'dropout_lstm_{i}', 0.1, 0.3, step=0.1)))\n",
    "    for i in range(hp.Int('num_dense_layers', 0, 1)):\n",
    "        if hp.Int('num_dense_layers', 0, 1) > 0:\n",
    "            model.add(layers.Dense(\n",
    "                units=hp.Int(f'units_dense_{i}', min_value=32, max_value=64, step=32),\n",
    "                activation='relu'\n",
    "            ))\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_dense_{i}', 0.1, 0.3, step=0.1)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 5e-4])),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Configurações ---\n",
    "\n",
    "tickers = [\"GGBR3.SA\"]\n",
    "pasta_dados_tratados = r\"C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\dataframesMicro\" \n",
    "\n",
    "start_date_val = \"2020-01-01\" \n",
    "end_date_val = \"2022-12-31\"  \n",
    "start_date_test = \"2023-01-01\"\n",
    "end_date_test = \"2023-12-31\"   \n",
    "\n",
    "janela_min = 1\n",
    "janela_max = 4\n",
    "n_splits_cv = 5\n",
    "max_trials_tuner = 10\n",
    "epochs_tuner = 50\n",
    "epochs_final = 100\n",
    "resultados_dir = \"resultados_lstm_cv_micro_tratados\" # Novo diretório de resultados\n",
    "os.makedirs(resultados_dir, exist_ok=True)\n",
    "\n",
    "# --- Função de Experimento Específico para MICRO ---\n",
    "def rodar_experimento_especifico_micro(ticker, janela, df_full, feature_cols, target_col):\n",
    "    \"\"\"\n",
    "    Roda um experimento completo para um ticker e janela específicos,\n",
    "    usando dados microeconômicos pré-carregados.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Iniciando Experimento com Dados Micro Tratados: {ticker}, Janela {janela} ---\")\n",
    "\n",
    "    base_filename = f\"{resultados_dir}/{ticker}_Janela_{janela}\"\n",
    "    metrics_path = f\"{base_filename}_metrics.csv\"\n",
    "    hiperparametros_path = f\"{base_filename}_hiperparametros.csv\"\n",
    "    grafico_path = f\"{base_filename}_grafico_teste_final.png\"\n",
    "    previsoes_path = f\"{base_filename}_previsoes_teste_final.csv\"\n",
    "\n",
    "    if os.path.exists(metrics_path):\n",
    "        print(f\"Resultados já existem para {ticker}, Janela {janela}. Pulando...\")\n",
    "        return\n",
    "\n",
    "    # 1. Separar Dados de Treino/Validação (2020-2022) e Teste (2023)\n",
    "    try:\n",
    "        df_val_train = df_full.loc[start_date_val:end_date_val].copy()\n",
    "        df_test_final = df_full.loc[start_date_test:end_date_test].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Erro ao dividir dados por data para {ticker}: {e}. Verifique as datas e o índice do DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if df_val_train.empty or df_test_final.empty:\n",
    "        print(f\"Erro: Período de treino/validação ou teste está vazio para {ticker}. Verifique as datas e os dados.\")\n",
    "        return\n",
    "\n",
    "    # Verificar se há NaNs nas features ou target após o split\n",
    "    if df_val_train[feature_cols].isnull().values.any() or df_val_train[target_col].isnull().values.any():\n",
    "        print(f\"Aviso: Encontrados NaNs nas features ou target de treino/validação para {ticker}. Verifique o pré-processamento.\")\n",
    "        return\n",
    "\n",
    "    if df_test_final[feature_cols].isnull().values.any() or df_test_final[target_col].isnull().values.any():\n",
    "        print(f\"Aviso: Encontrados NaNs nas features ou target de teste para {ticker}. Verifique o pré-processamento.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Separar features e target para treino/validação\n",
    "    features_val_train = df_val_train[feature_cols]\n",
    "    target_val_train = df_val_train[[target_col]] # Manter como DataFrame para o scaler\n",
    "\n",
    "    # 2. Escalonamento (DOIS SCALERS)\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler() # Scaler dedicado para o target ('Close_Target')\n",
    "\n",
    "    # Ajustar (fit) os scalers SOMENTE nos dados de treino/validação (2020-2022)\n",
    "    scaled_features_val_train = scaler_features.fit_transform(features_val_train)\n",
    "    # Ajustar e transformar o target T+1 usando o scaler dedicado\n",
    "    scaled_target_val_train = scaler_target.fit_transform(target_val_train)\n",
    "\n",
    "    num_features = scaled_features_val_train.shape[1]\n",
    "    print(f\"Número de features para o modelo: {num_features}\")\n",
    "\n",
    "    # 3. Criar Janelas Multivariadas para Treino/Validação\n",
    "    X_val_train_full, y_val_train_full = criar_janelas_multivariadas(\n",
    "        scaled_features_val_train,\n",
    "        scaled_target_val_train.flatten(), # Passar target escalado como array 1D\n",
    "        janela\n",
    "    )\n",
    "\n",
    "    if X_val_train_full.size == 0 or y_val_train_full.size == 0:\n",
    "        print(f\"Erro: Não foi possível criar janelas de treino/validação para {ticker}, Janela {janela}. Verifique o tamanho dos dados e a janela.\")\n",
    "        return\n",
    "\n",
    "    # 4. Otimização de Hiperparâmetros (Keras Tuner)\n",
    "    print(\"Iniciando busca de hiperparâmetros...\")\n",
    "    input_shape = (janela, num_features) # Forma da entrada para LSTM\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_model(hp, input_shape=input_shape),\n",
    "        objective='val_loss',\n",
    "        max_trials=max_trials_tuner,\n",
    "        executions_per_trial=1,\n",
    "        directory='keras_tuner_cv_micro_tratados', # Novo diretório\n",
    "        project_name=f'LSTM_{ticker}_Janela_{janela}',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    early_stopping_tuner = EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)\n",
    "\n",
    "    # Dividir para validação interna do tuner (respeitando ordem)\n",
    "    split_index = int(len(X_val_train_full) * 0.8)\n",
    "    X_tuner_train, y_tuner_train = X_val_train_full[:split_index], y_val_train_full[:split_index]\n",
    "    X_tuner_val, y_tuner_val = X_val_train_full[split_index:], y_val_train_full[split_index:]\n",
    "\n",
    "    if len(X_tuner_val) == 0:\n",
    "        print(f\"Aviso: Sem dados de validação suficientes para tuner em {ticker}, Janela {janela}. Usando treino completo para busca.\")\n",
    "        tuner.search(X_val_train_full, y_val_train_full, epochs=epochs_tuner, callbacks=[early_stopping_tuner], verbose=1)\n",
    "    else:\n",
    "        tuner.search(X_tuner_train, y_tuner_train, epochs=epochs_tuner,\n",
    "                     validation_data=(X_tuner_val, y_tuner_val),\n",
    "                     callbacks=[early_stopping_tuner], verbose=1)\n",
    "\n",
    "    try:\n",
    "        best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "        print(\"\\nMelhores Hiperparâmetros encontrados:\")\n",
    "        print(best_hps.values)\n",
    "    except IndexError:\n",
    "        print(f\"Erro: Keras Tuner não encontrou nenhum hiperparâmetro válido para {ticker}, Janela {janela}. Pulando para próximo.\")\n",
    "        return # Pula o resto do experimento se não houver HPs\n",
    "\n",
    "    # 5. Avaliação Cruzada dos Melhores HPs (TimeSeriesSplit no período 2020-2022)\n",
    "    print(f\"\\nIniciando TimeSeriesSplit CV ({n_splits_cv} folds) para avaliar os melhores HPs...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits_cv)\n",
    "    cv_metrics = {'mae': [], 'mse': [], 'rmse': [], 'r2': []}\n",
    "    fold = 0\n",
    "    for train_index, val_index in tscv.split(X_val_train_full): # Split nas janelas criadas\n",
    "        fold += 1\n",
    "        print(f\"   Fold {fold}/{n_splits_cv}...\")\n",
    "        X_train_fold, X_val_fold = X_val_train_full[train_index], X_val_train_full[val_index]\n",
    "        y_train_fold, y_val_fold = y_val_train_full[train_index], y_val_train_full[val_index]\n",
    "\n",
    "        # Verificar se há dados suficientes no fold\n",
    "        if len(X_train_fold) == 0 or len(X_val_fold) == 0:\n",
    "            print(f\"   Aviso: Fold {fold} tem 0 amostras de treino ou validação. Pulando fold.\")\n",
    "            continue\n",
    "\n",
    "        model_cv = tuner.hypermodel.build(best_hps)\n",
    "        early_stopping_cv = EarlyStopping(monitor='loss', patience=10, verbose=0)\n",
    "        model_cv.fit(X_train_fold, y_train_fold, epochs=epochs_final, batch_size=32,\n",
    "                     callbacks=[early_stopping_cv], verbose=0)\n",
    "\n",
    "        # Avaliar no conjunto de validação do fold\n",
    "        previsoes_val_fold_scaled = model_cv.predict(X_val_fold)\n",
    "        # INVERTER usando SCALER_TARGET\n",
    "        previsoes_val_fold = scaler_target.inverse_transform(previsoes_val_fold_scaled)\n",
    "        reais_val_fold_scaled = y_val_fold.reshape(-1, 1)\n",
    "        reais_val_fold = scaler_target.inverse_transform(reais_val_fold_scaled)\n",
    "\n",
    "        # Calcular métricas do fold\n",
    "        cv_metrics['mae'].append(mean_absolute_error(reais_val_fold, previsoes_val_fold))\n",
    "        cv_metrics['mse'].append(mean_squared_error(reais_val_fold, previsoes_val_fold))\n",
    "        cv_metrics['rmse'].append(np.sqrt(cv_metrics['mse'][-1]))\n",
    "        try:\n",
    "            r2_fold = r2_score(reais_val_fold, previsoes_val_fold)\n",
    "            cv_metrics['r2'].append(r2_fold if np.isfinite(r2_fold) else -1.0)\n",
    "        except ValueError:\n",
    "            cv_metrics['r2'].append(-1.0)\n",
    "\n",
    "        print(f\"     Fold {fold} MAE: {cv_metrics['mae'][-1]:.4f}, RMSE: {cv_metrics['rmse'][-1]:.4f}, R2: {cv_metrics['r2'][-1]:.4f}\")\n",
    "\n",
    "    # Calcular métricas médias da validação cruzada (apenas dos folds válidos)\n",
    "    if cv_metrics['mae']: # Verificar se algum fold foi executado\n",
    "        avg_cv_mae = np.mean(cv_metrics['mae'])\n",
    "        avg_cv_mse = np.mean(cv_metrics['mse'])\n",
    "        avg_cv_rmse = np.mean(cv_metrics['rmse'])\n",
    "        avg_cv_r2 = np.mean(cv_metrics['r2'])\n",
    "        print(\"\\nResultados Médios da Validação Cruzada (TimeSeriesSplit):\")\n",
    "        print(f\"   Avg MAE: {avg_cv_mae:.4f}, Avg MSE: {avg_cv_mse:.4f}, Avg RMSE: {avg_cv_rmse:.4f}, Avg R2: {avg_cv_r2:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nAviso: Nenhum fold da validação cruzada foi completado com sucesso.\")\n",
    "        avg_cv_mae, avg_cv_mse, avg_cv_rmse, avg_cv_r2 = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "\n",
    "    # 6. Treinamento Final (com melhores HPs, usando TODAS as janelas de 2020-2022)\n",
    "    print(\"\\nTreinando modelo final com melhores HPs...\")\n",
    "    model_final = tuner.hypermodel.build(best_hps)\n",
    "    early_stopping_final = EarlyStopping(monitor='loss', patience=15, verbose=1, restore_best_weights=True)\n",
    "    history = model_final.fit(X_val_train_full, y_val_train_full, epochs=epochs_final, batch_size=32,\n",
    "                              callbacks=[early_stopping_final], verbose=1)\n",
    "\n",
    "    # Verificar se o treinamento convergiu (loss diminuiu)\n",
    "    if not history.history or not history.history['loss'] or min(history.history['loss']) == history.history['loss'][0]:\n",
    "         print(f\"Aviso: Treinamento final para {ticker}, Janela {janela} pode não ter convergido bem (loss: {history.history['loss'][-1] if history.history['loss'] else 'N/A'}).\")\n",
    "\n",
    "\n",
    "    # 7. Avaliação Final no Conjunto de Teste (2023 - Hold-Out)\n",
    "    print(\"\\nAvaliando modelo final no conjunto de teste (2023)...\")\n",
    "\n",
    "    # Separar features e target para teste\n",
    "    features_test_final = df_test_final[feature_cols]\n",
    "    target_test_final = df_test_final[[target_col]] # DataFrame\n",
    "\n",
    "    # Escalar features de teste USANDO scaler_features AJUSTADO NO TREINO\n",
    "    scaled_features_test_final = scaler_features.transform(features_test_final)\n",
    "    # Escalar target de teste USANDO scaler_target AJUSTADO NO TREINO\n",
    "    scaled_target_test_final = scaler_target.transform(target_test_final)\n",
    "\n",
    "    # Criar janelas de teste\n",
    "    X_test_final, y_test_final = criar_janelas_multivariadas(\n",
    "        scaled_features_test_final,\n",
    "        scaled_target_test_final.flatten(),\n",
    "        janela\n",
    "    )\n",
    "\n",
    "    if X_test_final.size == 0 or y_test_final.size == 0:\n",
    "        print(f\"Erro: Não foi possível criar janelas de teste final para {ticker}, Janela {janela}. Verifique datas/janela/tamanho dos dados de teste.\")\n",
    "        # Salvar métricas parciais se a CV rodou\n",
    "        if not np.isnan(avg_cv_mae):\n",
    "            metrics_df = pd.DataFrame([{\n",
    "                'Ticker': ticker, 'Janela': janela, 'Num Features': num_features,\n",
    "                'CV Avg MAE': avg_cv_mae, 'CV Avg MSE': avg_cv_mse, 'CV Avg RMSE': avg_cv_rmse, 'CV Avg R2': avg_cv_r2,\n",
    "                'Teste MAE': np.nan, 'Teste MSE': np.nan, 'Teste RMSE': np.nan, 'Teste R2': np.nan\n",
    "             }])\n",
    "            metrics_df.to_csv(metrics_path, index=False)\n",
    "            print(f\"Métricas parciais (CV) salvas em {metrics_path}\")\n",
    "        return # Não continuar se não houver janelas de teste\n",
    "\n",
    "    # Fazer previsões (escaladas)\n",
    "    previsoes_test_scaled = model_final.predict(X_test_final)\n",
    "\n",
    "    # INVERTER usando SCALER_TARGET\n",
    "    previsoes_test_final = scaler_target.inverse_transform(previsoes_test_scaled)\n",
    "    reais_test_final_scaled = y_test_final.reshape(-1, 1)\n",
    "    reais_test_final = scaler_target.inverse_transform(reais_test_final_scaled)\n",
    "\n",
    "    # Calcular métricas do teste final\n",
    "    mae_test = mean_absolute_error(reais_test_final, previsoes_test_final)\n",
    "    mse_test = mean_squared_error(reais_test_final, previsoes_test_final)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_test = r2_score(reais_test_final, previsoes_test_final)\n",
    "    print(\"\\nResultados da Avaliação Final (Teste 2023):\")\n",
    "    print(f\"   MAE Teste: {mae_test:.4f}, MSE Teste: {mse_test:.4f}, RMSE Teste: {rmse_test:.4f}, R² Teste: {r2_test:.4f}\")\n",
    "\n",
    "    # 8. Salvar Resultados\n",
    "    print(\"\\nSalvando resultados...\")\n",
    "    # Métricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'Ticker': ticker, 'Janela': janela, 'Num Features': num_features,\n",
    "        'CV Avg MAE': avg_cv_mae, 'CV Avg MSE': avg_cv_mse, 'CV Avg RMSE': avg_cv_rmse, 'CV Avg R2': avg_cv_r2,\n",
    "        'Teste MAE': mae_test, 'Teste MSE': mse_test, 'Teste RMSE': rmse_test, 'Teste R2': r2_test\n",
    "    }])\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"Métricas salvas em {metrics_path}\")\n",
    "\n",
    "    # Hiperparâmetros\n",
    "    hiperparametros_df = pd.DataFrame([best_hps.values])\n",
    "    hiperparametros_df.to_csv(hiperparametros_path, index=False)\n",
    "    print(f\"Hiperparâmetros salvos em {hiperparametros_path}\")\n",
    "\n",
    "    # Previsões do teste final\n",
    "    # As datas corretas para y_test_final são as datas do df_test_final a partir da posição 'janela'\n",
    "    # Pois a primeira janela usa dados de 0 a janela-1 para prever o target em 'janela'\n",
    "    datas_teste_final = df_test_final.index[janela:]\n",
    "\n",
    "    # Verificar se o número de datas corresponde ao número de previsões/reais\n",
    "    if len(datas_teste_final) == len(reais_test_final) and len(datas_teste_final) == len(previsoes_test_final):\n",
    "        previsoes_df = pd.DataFrame({\n",
    "            'Data': datas_teste_final,\n",
    "            'Preço Real': reais_test_final.flatten(),\n",
    "            'Preço Previsto': previsoes_test_final.flatten()\n",
    "        })\n",
    "        previsoes_df.to_csv(previsoes_path, index=False)\n",
    "        print(f\"Previsões do teste salvas em {previsoes_path}\")\n",
    "\n",
    "        # Gerar gráfico\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Real'], label='Real (Teste 2023)', color='blue', linewidth=1.5)\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Previsto'], label='Previsto (Teste 2023)', color='orange', linestyle='--', linewidth=1.5)\n",
    "        plt.title(f'Preços Reais vs Previstos (Teste Final com Dados Micro) - {ticker} (Janela {janela})')\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Preço')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.4)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(grafico_path)\n",
    "        print(f\"Gráfico do teste salvo em {grafico_path}\")\n",
    "        plt.close() # Fechar a figura para liberar memória\n",
    "    else:\n",
    "        print(f\"Erro: Discrepância entre número de datas ({len(datas_teste_final)}) e previsões/reais ({len(reais_test_final)}/{len(previsoes_test_final)}) para {ticker}, Janela {janela} no teste final.\")\n",
    "        print(f\"   Índice df_test_final começa: {df_test_final.index.min()}, termina: {df_test_final.index.max()}, len: {len(df_test_final)}\")\n",
    "        print(f\"   Índice datas_teste_final começa: {datas_teste_final.min() if not datas_teste_final.empty else 'N/A'}, termina: {datas_teste_final.max() if not datas_teste_final.empty else 'N/A'}, len: {len(datas_teste_final)}\")\n",
    "        print(f\"   Tamanho X_test_final: {X_test_final.shape}, y_test_final: {y_test_final.shape}, previsoes_test_final: {previsoes_test_final.shape}\")\n",
    "        print(\"   Gráfico/Previsões não salvos devido à inconsistência.\")\n",
    "\n",
    "    print(f\"--- Experimento Concluído: {ticker}, Janela {janela} ---\")\n",
    "\n",
    "\n",
    "# --- Loop Principal de Experimentos para MICRO ---\n",
    "def rodar_experimentos_cv_micro_tratados():\n",
    "    for ticker in tickers:\n",
    "        # Carregar os dados para o ticker atual\n",
    "        df_ticker_full, feature_cols, target_col = carregar_dados_ticker_micro(ticker, pasta_dados_tratados)\n",
    "\n",
    "        if df_ticker_full is None:\n",
    "            print(f\"Não foi possível carregar dados para {ticker}. Pulando ticker.\")\n",
    "            continue # Pula para o próximo ticker se os dados não puderam ser carregados\n",
    "\n",
    "        # Iterar pelas janelas para o ticker atual\n",
    "        for janela in range(janela_min, janela_max + 1):\n",
    "            try:\n",
    "                # Passar o DataFrame completo e as colunas identificadas\n",
    "                rodar_experimento_especifico_micro(ticker, janela, df_ticker_full, feature_cols, target_col)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro CRÍTICO ao rodar experimento {ticker}, Janela {janela}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # Imprime o stack trace completo para depuração\n",
    "\n",
    "# --- Rodar os experimentos ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Verificar se a pasta de dados existe\n",
    "    if not os.path.isdir(pasta_dados_tratados):\n",
    "         print(f\"ERRO FATAL: A pasta de dados micro tratados '{pasta_dados_tratados}' não foi encontrada.\")\n",
    "         print(\"Por favor, verifique o caminho na variável 'pasta_dados_tratados'.\")\n",
    "    else:\n",
    "        rodar_experimentos_cv_micro_tratados()\n",
    "        print(\"\\n--- TODOS OS EXPERIMENTOS COM DADOS MICRO TRATADOS CONCLUÍDOS ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
