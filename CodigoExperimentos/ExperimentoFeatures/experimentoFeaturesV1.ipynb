{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando experimentos com modelos melhorados para features macro/micro...\n",
      "Carregando dados tratados de: C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\dados_unificados\\GGBR3.SA_dados_macro_micro.csv\n",
      "Colunas originais: ['Preço', 'Close_Feature', 'Close_Target', 'ROA', 'ROE', 'Margem Líquida', 'P/L', 'VP', 'Preço_anterior', 'TaxaCambio', 'Selic', 'PIB', 'IPCA', \"('GGBR3.SA_Close', 'GGBR3.SA')\"]\n",
      "Features identificadas para GGBR3.SA: ['Close_Feature', 'TaxaCambio', 'Selic', 'PIB', 'IPCA', 'ROA', 'ROE', 'Margem Líquida', 'P/L', 'VP']\n",
      "Target identificado para GGBR3.SA: Close_Target\n",
      "Dimensões finais do DataFrame para GGBR3.SA: (1460, 11)\n",
      "\n",
      "--- Iniciando Experimento: GGBR3.SA, Janela 1 ---\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_14240\\1233118696.py:93: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[feature_cols] = df[feature_cols].fillna(method='ffill')\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_14240\\1233118696.py:95: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[feature_cols] = df[feature_cols].fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.6097 - val_loss: 0.0461 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0708 - val_loss: 0.0432 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0538 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0427 - val_loss: 0.0716 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0339 - val_loss: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0289 - val_loss: 0.0592 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0258 - val_loss: 0.0589 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0222 - val_loss: 0.0455 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0224 - val_loss: 0.0572 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0219 - val_loss: 0.0358 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - val_loss: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0226 - val_loss: 0.0411 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0198 - val_loss: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - val_loss: 0.0465 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0161 - val_loss: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0194 - val_loss: 0.0369 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0189 - val_loss: 0.0504 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0157 - val_loss: 0.0580 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - val_loss: 0.0572 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m26/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0181 \n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0181 - val_loss: 0.0562 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0167 - val_loss: 0.0532 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - val_loss: 0.0460 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0147 - val_loss: 0.0483 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0174 - val_loss: 0.0566 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0150 - val_loss: 0.0507 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0169 - val_loss: 0.0592 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0164 - val_loss: 0.0541 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0155 - val_loss: 0.0630 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - val_loss: 0.0522 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m25/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0151 \n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0153 - val_loss: 0.0505 - learning_rate: 5.0000e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Resultados da Avaliação:\n",
      "MAE: 0.9458, MSE: 1.3844, RMSE: 1.1766, R²: 0.4090\n",
      "Variação Real: 0.0786, Variação Prevista: 0.0486\n",
      "Previsões travadas? Não\n",
      "--- Experimento concluído: GGBR3.SA, Janela 1 ---\n",
      "\n",
      "--- Iniciando Experimento: GGBR3.SA, Janela 2 ---\n",
      "Epoch 1/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.6511 - val_loss: 0.3806 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0665 - val_loss: 0.1829 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.1510 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0360 - val_loss: 0.1320 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0288 - val_loss: 0.1182 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0268 - val_loss: 0.1030 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0254 - val_loss: 0.1062 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0220 - val_loss: 0.0989 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0223 - val_loss: 0.0882 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0216 - val_loss: 0.0850 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0191 - val_loss: 0.0889 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0209 - val_loss: 0.0820 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - val_loss: 0.0869 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - val_loss: 0.0826 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0173 - val_loss: 0.0887 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0171 - val_loss: 0.0750 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0160 - val_loss: 0.0785 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0170 - val_loss: 0.0769 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0156 - val_loss: 0.0795 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0174 - val_loss: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0175 - val_loss: 0.0905 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0188 - val_loss: 0.0882 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0170 - val_loss: 0.0891 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0165 - val_loss: 0.0971 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0193 - val_loss: 0.0896 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m21/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0181 \n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0180 - val_loss: 0.0898 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0162 - val_loss: 0.0913 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0162 - val_loss: 0.0935 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0167 - val_loss: 0.0900 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0160 - val_loss: 0.0948 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0149 - val_loss: 0.0947 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0141 - val_loss: 0.0951 - learning_rate: 5.0000e-04\n",
      "Epoch 33/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0156 - val_loss: 0.0902 - learning_rate: 5.0000e-04\n",
      "Epoch 34/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0156 - val_loss: 0.0980 - learning_rate: 5.0000e-04\n",
      "Epoch 35/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0140 - val_loss: 0.1026 - learning_rate: 5.0000e-04\n",
      "Epoch 36/200\n",
      "\u001b[1m20/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0160 \n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0161 - val_loss: 0.0931 - learning_rate: 5.0000e-04\n",
      "Epoch 36: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "Resultados da Avaliação:\n",
      "MAE: 2.2136, MSE: 5.6699, RMSE: 2.3812, R²: -1.4195\n",
      "Variação Real: 0.0786, Variação Prevista: 0.0416\n",
      "Previsões travadas? Não\n",
      "--- Experimento concluído: GGBR3.SA, Janela 2 ---\n",
      "\n",
      "--- Iniciando Experimento: GGBR3.SA, Janela 3 ---\n",
      "Epoch 1/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.5873 - val_loss: 0.2067 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0652 - val_loss: 0.1449 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0432 - val_loss: 0.1460 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0375 - val_loss: 0.1149 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0322 - val_loss: 0.1299 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0333 - val_loss: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0256 - val_loss: 0.1211 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0234 - val_loss: 0.1301 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0220 - val_loss: 0.1006 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0204 - val_loss: 0.1110 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0226 - val_loss: 0.1017 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0234 - val_loss: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0215 - val_loss: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0199 - val_loss: 0.1236 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0201 - val_loss: 0.1027 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0241 - val_loss: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0208 - val_loss: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0187 - val_loss: 0.1112 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m17/28\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 \n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0208 - val_loss: 0.1191 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0181 - val_loss: 0.1104 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0158 - val_loss: 0.1189 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0160 - val_loss: 0.1368 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0202 - val_loss: 0.1196 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0169 - val_loss: 0.1199 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0157 - val_loss: 0.1199 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0163 - val_loss: 0.1256 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0173 - val_loss: 0.1224 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0159 - val_loss: 0.1078 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m21/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0174\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0172 - val_loss: 0.1328 - learning_rate: 5.0000e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "\n",
      "Resultados da Avaliação:\n",
      "MAE: 1.6880, MSE: 3.6715, RMSE: 1.9161, R²: -0.5665\n",
      "Variação Real: 0.0786, Variação Prevista: 0.0428\n",
      "Previsões travadas? Não\n",
      "--- Experimento concluído: GGBR3.SA, Janela 3 ---\n",
      "\n",
      "--- Iniciando Experimento: GGBR3.SA, Janela 4 ---\n",
      "Epoch 1/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.4306 - val_loss: 0.2758 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0592 - val_loss: 0.2166 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0450 - val_loss: 0.1954 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0361 - val_loss: 0.1892 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0330 - val_loss: 0.1537 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0297 - val_loss: 0.1482 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0296 - val_loss: 0.1385 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0264 - val_loss: 0.1260 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0272 - val_loss: 0.1133 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0239 - val_loss: 0.1130 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0223 - val_loss: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0215 - val_loss: 0.1096 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0217 - val_loss: 0.1336 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0240 - val_loss: 0.1323 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0194 - val_loss: 0.1324 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0217 - val_loss: 0.1432 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0235 - val_loss: 0.1398 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0198 - val_loss: 0.1469 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0215 - val_loss: 0.1589 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0205 - val_loss: 0.1653 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0190 - val_loss: 0.1421 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m27/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0194\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0194 - val_loss: 0.1317 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0158 - val_loss: 0.1566 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0167 - val_loss: 0.1501 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0169 - val_loss: 0.1377 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0173 - val_loss: 0.1411 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0143 - val_loss: 0.1461 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0177 - val_loss: 0.1449 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0151 - val_loss: 0.1426 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0185 - val_loss: 0.1532 - learning_rate: 5.0000e-04\n",
      "Epoch 31/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0194 - val_loss: 0.1512 - learning_rate: 5.0000e-04\n",
      "Epoch 32/200\n",
      "\u001b[1m15/28\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0149 \n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0158 - val_loss: 0.1501 - learning_rate: 5.0000e-04\n",
      "Epoch 32: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Resultados da Avaliação:\n",
      "MAE: 2.1696, MSE: 5.6021, RMSE: 2.3669, R²: -1.3890\n",
      "Variação Real: 0.0787, Variação Prevista: 0.0423\n",
      "Previsões travadas? Não\n",
      "--- Experimento concluído: GGBR3.SA, Janela 4 ---\n",
      "\n",
      "--- Iniciando Experimento: GGBR3.SA, Janela 5 ---\n",
      "Epoch 1/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 0.5215 - val_loss: 0.3143 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0837 - val_loss: 0.2681 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0455 - val_loss: 0.2047 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0414 - val_loss: 0.1933 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0319 - val_loss: 0.1754 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0313 - val_loss: 0.1434 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0340 - val_loss: 0.1282 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0288 - val_loss: 0.0956 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0276 - val_loss: 0.0894 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0225 - val_loss: 0.0767 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0258 - val_loss: 0.0795 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0235 - val_loss: 0.0799 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0193 - val_loss: 0.1084 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0209 - val_loss: 0.0935 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0204 - val_loss: 0.0944 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0217 - val_loss: 0.1003 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0233 - val_loss: 0.1025 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0217 - val_loss: 0.0917 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0231 - val_loss: 0.0784 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m27/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0219\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0218 - val_loss: 0.1102 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0162 - val_loss: 0.0919 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0163 - val_loss: 0.0934 - learning_rate: 5.0000e-04\n",
      "Epoch 23/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0153 - val_loss: 0.0888 - learning_rate: 5.0000e-04\n",
      "Epoch 24/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0165 - val_loss: 0.0803 - learning_rate: 5.0000e-04\n",
      "Epoch 25/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0174 - val_loss: 0.0876 - learning_rate: 5.0000e-04\n",
      "Epoch 26/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0173 - val_loss: 0.0930 - learning_rate: 5.0000e-04\n",
      "Epoch 27/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0181 - val_loss: 0.0928 - learning_rate: 5.0000e-04\n",
      "Epoch 28/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0169 - val_loss: 0.0934 - learning_rate: 5.0000e-04\n",
      "Epoch 29/200\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0177 - val_loss: 0.0946 - learning_rate: 5.0000e-04\n",
      "Epoch 30/200\n",
      "\u001b[1m27/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0187\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0187 - val_loss: 0.0889 - learning_rate: 5.0000e-04\n",
      "Epoch 30: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "Resultados da Avaliação:\n",
      "MAE: 2.3526, MSE: 6.3649, RMSE: 2.5229, R²: -1.7184\n",
      "Variação Real: 0.0786, Variação Prevista: 0.0487\n",
      "Previsões travadas? Não\n",
      "--- Experimento concluído: GGBR3.SA, Janela 5 ---\n",
      "\n",
      "Resultados consolidados salvos em C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\resultados_lstm_macromicro_melhorado/resultados_consolidados.csv\n",
      "\n",
      "Todos experimentos concluídos!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler  # Mudança para StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import traceback\n",
    "\n",
    "# Configurar o TensorFlow para usar menos memória\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- Função para carregar dados ---\n",
    "def carregar_dados_ticker(ticker, pasta_dados_tratados):\n",
    "    \"\"\"Carrega dados pré-processados, renomeia colunas e trata NaNs.\"\"\"\n",
    "    nome_arquivo = f\"{ticker}_dados_macro_micro.csv\"\n",
    "    caminho_arquivo = os.path.join(pasta_dados_tratados, nome_arquivo)\n",
    "\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        print(f\"Erro: Arquivo não encontrado para {ticker} em {caminho_arquivo}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        print(f\"Carregando dados tratados de: {caminho_arquivo}\")\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, index_col=0)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar arquivo: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Colunas originais:\", df.columns.tolist())\n",
    "\n",
    "        # Identificar colunas de preço alvo\n",
    "        target_col = None\n",
    "        if 'Close_Target' in df.columns:\n",
    "            target_col = 'Close_Target'\n",
    "        elif 'Preço' in df.columns:\n",
    "            target_col = 'Preço'\n",
    "        else:\n",
    "            print(f\"Erro: Nenhuma coluna de preço alvo encontrada para {ticker}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Identificar a coluna de feature de preço\n",
    "        feature_cols = []\n",
    "        price_feature_col = None\n",
    "        \n",
    "        if 'Close_Feature' in df.columns:\n",
    "            price_feature_col = 'Close_Feature'\n",
    "        elif 'Preço_anterior' in df.columns:\n",
    "            price_feature_col = 'Preço_anterior'\n",
    "        else:\n",
    "            print(f\"Aviso: Nenhuma coluna de preço anterior encontrada para {ticker}\")\n",
    "        \n",
    "        if price_feature_col:\n",
    "            feature_cols.append(price_feature_col)\n",
    "\n",
    "        # Separar features macro e micro\n",
    "        macro_cols = ['TaxaCambio', 'Selic', 'PIB', 'IPCA']\n",
    "        micro_cols = ['ROA', 'ROE', 'Margem Líquida', 'P/L', 'VP']\n",
    "\n",
    "        # Adicionar features disponíveis\n",
    "        for col in macro_cols + micro_cols:\n",
    "            if col in df.columns:\n",
    "                feature_cols.append(col)\n",
    "\n",
    "        # Verificar se temos features suficientes\n",
    "        if len(feature_cols) < 2:  # Pelo menos uma feature além do preço\n",
    "            print(f\"Aviso: Poucas features encontradas para {ticker}: {feature_cols}\")\n",
    "\n",
    "        print(f\"Features identificadas para {ticker}: {feature_cols}\")\n",
    "        print(f\"Target identificado para {ticker}: {target_col}\")\n",
    "\n",
    "        # Tratar NaNs\n",
    "        df = df[feature_cols + [target_col]].copy()\n",
    "        df.dropna(subset=[target_col], inplace=True)  # Remove linhas onde o target é NaN\n",
    "        \n",
    "        # Garantir features numéricas\n",
    "        for col in feature_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Preencher NaNs nas features\n",
    "        df[feature_cols] = df[feature_cols].fillna(method='ffill')\n",
    "        if df[feature_cols].isnull().any().any():\n",
    "            df[feature_cols] = df[feature_cols].fillna(method='bfill')\n",
    "        \n",
    "        # Verificar se ainda há NaNs\n",
    "        if df[feature_cols + [target_col]].isnull().any().any():\n",
    "            print(f\"Aviso: Ainda existem NaNs para {ticker}. Removendo linhas afetadas...\")\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "        print(f\"Dimensões finais do DataFrame para {ticker}: {df.shape}\")\n",
    "        if df.empty:\n",
    "            print(f\"Erro: DataFrame ficou vazio para {ticker} após limpeza.\")\n",
    "            return None, None, None\n",
    "\n",
    "        return df, feature_cols, target_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro CRÍTICO ao carregar/processar {caminho_arquivo}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# --- Função para criar janelas multivariadas com diferenciação ---\n",
    "def criar_janelas_multivariadas_diff(features_array, target_array, janela, add_diff=True):\n",
    "    \"\"\"Cria janelas multivariadas com diferenciação para melhorar previsões\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    if add_diff and features_array.shape[1] > 0:\n",
    "        # Adicionar colunas de diferenciação para cada feature\n",
    "        diffs = np.diff(features_array, axis=0)\n",
    "        # Concatenar com zeros no início para manter dimensões\n",
    "        zeros_row = np.zeros((1, diffs.shape[1]))\n",
    "        diffs_padded = np.vstack([zeros_row, diffs])\n",
    "        \n",
    "        # Combinar features originais com suas diferenciações\n",
    "        features_array = np.concatenate([features_array, diffs_padded], axis=1)\n",
    "    \n",
    "    if len(features_array) <= janela:\n",
    "        print(f\"Aviso: dados insuficientes ({len(features_array)}) para janela ({janela}).\")\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    for i in range(len(features_array) - janela):\n",
    "        X.append(features_array[i:(i + janela), :])\n",
    "        y.append(target_array[i + janela])  # Target já é T+1\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- Modelo LSTM melhorado com Residual Connections ---\n",
    "def build_model_improved(input_shape, dropout_rate=0.2):\n",
    "    \"\"\"Constrói modelo LSTM com conexões residuais para evitar previsões travadas\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Camada LSTM 1\n",
    "    lstm1 = layers.LSTM(100, return_sequences=True)(inputs)\n",
    "    lstm1 = layers.Dropout(dropout_rate)(lstm1)\n",
    "    \n",
    "    # Camada LSTM 2 com conexão residual\n",
    "    lstm2 = layers.LSTM(100, return_sequences=False)(lstm1)\n",
    "    lstm2 = layers.Dropout(dropout_rate)(lstm2)\n",
    "    \n",
    "    # Atenção à última dimensão da entrada para criar conexão residual\n",
    "    input_flattened = layers.Flatten()(inputs)\n",
    "    input_dense = layers.Dense(100)(input_flattened)\n",
    "    \n",
    "    # Combinar saída do LSTM e entrada (conexão residual)\n",
    "    combined = layers.Add()([lstm2, input_dense])\n",
    "    combined = layers.Activation('relu')(combined)\n",
    "    \n",
    "    # Camadas densas finais\n",
    "    dense1 = layers.Dense(64, activation='relu')(combined)\n",
    "    dense1 = layers.Dropout(dropout_rate/2)(dense1)\n",
    "    \n",
    "    # Saída\n",
    "    outputs = layers.Dense(1)(dense1)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Função principal para treinar e avaliar o modelo ---\n",
    "def treinar_avaliar_modelo(ticker, janela, df_full, feature_cols, target_col, \n",
    "                           resultados_dir=\"resultados_lstm_macromicro_melhorado\"):\n",
    "    \"\"\"Treina e avalia o modelo LSTM melhorado para um ticker e janela\"\"\"\n",
    "    print(f\"\\n--- Iniciando Experimento: {ticker}, Janela {janela} ---\")\n",
    "    \n",
    "    os.makedirs(resultados_dir, exist_ok=True)\n",
    "    base_filename = f\"{resultados_dir}/{ticker}_Janela_{janela}\"\n",
    "    metrics_path = f\"{base_filename}_metrics.csv\"\n",
    "    grafico_path = f\"{base_filename}_grafico_teste_final.png\"\n",
    "    previsoes_path = f\"{base_filename}_previsoes_teste_final.csv\"\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        print(f\"Resultados já existem para {ticker}, Janela {janela}. Pulando...\")\n",
    "        return\n",
    "    \n",
    "    # 1. Separar Dados Treino/Validação (2020-2022) e Teste (2023)\n",
    "    start_date_val = \"2020-01-01\"\n",
    "    end_date_val = \"2022-12-31\"\n",
    "    start_date_test = \"2023-01-01\"\n",
    "    end_date_test = \"2023-12-31\"\n",
    "    \n",
    "    try:\n",
    "        df_val_train = df_full.loc[start_date_val:end_date_val].copy()\n",
    "        df_test_final = df_full.loc[start_date_test:end_date_test].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Erro ao dividir dados por data para {ticker}: {e}.\")\n",
    "        if not df_full.empty: \n",
    "            print(f\"Datas disponíveis: {df_full.index.min()} a {df_full.index.max()}\")\n",
    "        return\n",
    "    \n",
    "    if df_val_train.empty or df_test_final.empty:\n",
    "        print(f\"Erro: Período treino/val ou teste vazio para {ticker}.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Preparar dados\n",
    "    features_val_train = df_val_train[feature_cols].values\n",
    "    target_val_train = df_val_train[target_col].values\n",
    "    \n",
    "    features_test = df_test_final[feature_cols].values\n",
    "    target_test = df_test_final[target_col].values\n",
    "    \n",
    "    # 3. Escalonamento com StandardScaler (em vez de MinMaxScaler)\n",
    "    scaler_features = StandardScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "    \n",
    "    # Ajustar escaladores apenas nos dados de treino\n",
    "    scaled_features_val_train = scaler_features.fit_transform(features_val_train)\n",
    "    target_val_train_reshaped = target_val_train.reshape(-1, 1)\n",
    "    scaled_target_val_train = scaler_target.fit_transform(target_val_train_reshaped).flatten()\n",
    "    \n",
    "    # Transformar dados de teste\n",
    "    scaled_features_test = scaler_features.transform(features_test)\n",
    "    target_test_reshaped = target_test.reshape(-1, 1)\n",
    "    scaled_target_test = scaler_target.transform(target_test_reshaped).flatten()\n",
    "    \n",
    "    # 4. Criar janelas com diferenciação\n",
    "    X_train, y_train = criar_janelas_multivariadas_diff(\n",
    "        scaled_features_val_train, \n",
    "        scaled_target_val_train, \n",
    "        janela,\n",
    "        add_diff=True  # Adicionar colunas de diferenciação\n",
    "    )\n",
    "    \n",
    "    X_test, y_test = criar_janelas_multivariadas_diff(\n",
    "        scaled_features_test, \n",
    "        scaled_target_test, \n",
    "        janela,\n",
    "        add_diff=True  # Adicionar colunas de diferenciação\n",
    "    )\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"Erro: Não foi possível criar janelas para {ticker}.\")\n",
    "        return\n",
    "    \n",
    "    # 5. Configurar validação\n",
    "    val_split_idx = int(0.8 * len(X_train))\n",
    "    X_train_final, y_train_final = X_train[:val_split_idx], y_train[:val_split_idx]\n",
    "    X_val, y_val = X_train[val_split_idx:], y_train[val_split_idx:]\n",
    "    \n",
    "    # 6. Construir e treinar modelo\n",
    "    input_shape = (janela, X_train.shape[2])\n",
    "    model = build_model_improved(input_shape)\n",
    "    \n",
    "    # Callbacks para melhorar treinamento\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=20, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    history = model.fit(\n",
    "        X_train_final, y_train_final,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 7. Avaliar no conjunto de teste\n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    predictions = scaler_target.inverse_transform(predictions_scaled).flatten()\n",
    "    actual = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 8. Calcular métricas\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    \n",
    "    # 9. Verificar variação das previsões (para detectar previsões travadas)\n",
    "    var_real = np.std(actual) / np.mean(actual) if np.mean(actual) != 0 else 0\n",
    "    var_pred = np.std(predictions) / np.mean(predictions) if np.mean(predictions) != 0 else 0\n",
    "    prop_var = var_pred / var_real if var_real != 0 else 0\n",
    "    travado = \"Sim\" if prop_var < 0.3 else \"Não\"\n",
    "    \n",
    "    print(\"\\nResultados da Avaliação:\")\n",
    "    print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    print(f\"Variação Real: {var_real:.4f}, Variação Prevista: {var_pred:.4f}\")\n",
    "    print(f\"Previsões travadas? {travado}\")\n",
    "    \n",
    "    # 10. Salvar resultados\n",
    "    # Métricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'Ticker': ticker,\n",
    "        'Janela': janela,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Var_Real': var_real,\n",
    "        'Var_Pred': var_pred,\n",
    "        'Prop_Var': prop_var,\n",
    "        'Travado': travado\n",
    "    }])\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    \n",
    "    # Datas para as previsões\n",
    "    datas_teste = df_test_final.index[janela:janela+len(actual)]\n",
    "    \n",
    "    if len(datas_teste) == len(actual):\n",
    "        # Salvar previsões\n",
    "        previsoes_df = pd.DataFrame({\n",
    "            'Data': datas_teste,\n",
    "            'Preço Real': actual,\n",
    "            'Preço Previsto': predictions\n",
    "        })\n",
    "        previsoes_df.to_csv(previsoes_path, index=False)\n",
    "        \n",
    "        # Gerar gráfico\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Real'], label='Real', color='blue', linewidth=1.5)\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Previsto'], label='Previsto', color='orange', linestyle='--', linewidth=1.5)\n",
    "        plt.title(f'Preços Reais vs Previstos - {ticker} (Janela {janela})')\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Preço')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.4)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(grafico_path)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"--- Experimento concluído: {ticker}, Janela {janela} ---\")\n",
    "    return metrics_df\n",
    "\n",
    "# --- Função para rodar experimentos em todos os tickers ---\n",
    "def rodar_experimentos_macromicro_melhorados():\n",
    "    pasta_dados = r\"C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\dados_unificados\"\n",
    "    resultados_dir = r\"C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\resultados_lstm_macromicro_melhorado\"\n",
    "    os.makedirs(resultados_dir, exist_ok=True)\n",
    "    \n",
    "    # Lista de tickers\n",
    "    tickers = [\"GGBR3.SA\"]\n",
    "    \n",
    "    # Janelas a testar\n",
    "    janelas = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    resultados_consolidados = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        df_ticker_full, feature_cols, target_col = carregar_dados_ticker(ticker, pasta_dados)\n",
    "        \n",
    "        if df_ticker_full is None:\n",
    "            print(f\"Pulando ticker {ticker} devido a erro no carregamento.\")\n",
    "            continue\n",
    "        \n",
    "        for janela in janelas:\n",
    "            try:\n",
    "                metrics = treinar_avaliar_modelo(ticker, janela, df_ticker_full, feature_cols, target_col, resultados_dir)\n",
    "                if metrics is not None:\n",
    "                    resultados_consolidados.append(metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {ticker}, Janela {janela}: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Consolidar resultados\n",
    "    if resultados_consolidados:\n",
    "        df_resultados = pd.concat(resultados_consolidados)\n",
    "        df_resultados.to_csv(f\"{resultados_dir}/resultados_consolidados.csv\", index=False)\n",
    "        print(f\"\\nResultados consolidados salvos em {resultados_dir}/resultados_consolidados.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Desativar mensagens de aviso do TensorFlow\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    print(\"Iniciando experimentos com modelos melhorados para features macro/micro...\")\n",
    "    rodar_experimentos_macromicro_melhorados()\n",
    "    print(\"\\nTodos experimentos concluídos!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
