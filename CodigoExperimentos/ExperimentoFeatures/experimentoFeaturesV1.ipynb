{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler  \u001b[38;5;66;03m# Mudança para StandardScaler\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import traceback\n",
    "\n",
    "# Configurar o TensorFlow para usar menos memória\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# --- Função para carregar dados ---\n",
    "def carregar_dados_ticker(ticker, pasta_dados_tratados):\n",
    "    \"\"\"Carrega dados pré-processados, renomeia colunas e trata NaNs.\"\"\"\n",
    "    nome_arquivo = f\"{ticker}_dados_macro_micro.csv\"\n",
    "    caminho_arquivo = os.path.join(pasta_dados_tratados, nome_arquivo)\n",
    "\n",
    "    if not os.path.exists(caminho_arquivo):\n",
    "        print(f\"Erro: Arquivo não encontrado para {ticker} em {caminho_arquivo}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        print(f\"Carregando dados tratados de: {caminho_arquivo}\")\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, index_col=0)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar arquivo: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "        print(\"Colunas originais:\", df.columns.tolist())\n",
    "\n",
    "        # Identificar colunas de preço alvo\n",
    "        target_col = None\n",
    "        if 'Close_Target' in df.columns:\n",
    "            target_col = 'Close_Target'\n",
    "        elif 'Preço' in df.columns:\n",
    "            target_col = 'Preço'\n",
    "        else:\n",
    "            print(f\"Erro: Nenhuma coluna de preço alvo encontrada para {ticker}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Identificar a coluna de feature de preço\n",
    "        feature_cols = []\n",
    "        price_feature_col = None\n",
    "        \n",
    "        if 'Close_Feature' in df.columns:\n",
    "            price_feature_col = 'Close_Feature'\n",
    "        elif 'Preço_anterior' in df.columns:\n",
    "            price_feature_col = 'Preço_anterior'\n",
    "        else:\n",
    "            print(f\"Aviso: Nenhuma coluna de preço anterior encontrada para {ticker}\")\n",
    "        \n",
    "        if price_feature_col:\n",
    "            feature_cols.append(price_feature_col)\n",
    "\n",
    "        # Separar features macro e micro\n",
    "        macro_cols = ['TaxaCambio', 'Selic', 'PIB', 'IPCA']\n",
    "        micro_cols = ['ROA', 'ROE', 'Margem Líquida', 'P/L', 'VP']\n",
    "\n",
    "        # Adicionar features disponíveis\n",
    "        for col in macro_cols + micro_cols:\n",
    "            if col in df.columns:\n",
    "                feature_cols.append(col)\n",
    "\n",
    "        # Verificar se temos features suficientes\n",
    "        if len(feature_cols) < 2:  # Pelo menos uma feature além do preço\n",
    "            print(f\"Aviso: Poucas features encontradas para {ticker}: {feature_cols}\")\n",
    "\n",
    "        print(f\"Features identificadas para {ticker}: {feature_cols}\")\n",
    "        print(f\"Target identificado para {ticker}: {target_col}\")\n",
    "\n",
    "        # Tratar NaNs\n",
    "        df = df[feature_cols + [target_col]].copy()\n",
    "        df.dropna(subset=[target_col], inplace=True)  # Remove linhas onde o target é NaN\n",
    "        \n",
    "        # Garantir features numéricas\n",
    "        for col in feature_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Preencher NaNs nas features\n",
    "        df[feature_cols] = df[feature_cols].fillna(method='ffill')\n",
    "        if df[feature_cols].isnull().any().any():\n",
    "            df[feature_cols] = df[feature_cols].fillna(method='bfill')\n",
    "        \n",
    "        # Verificar se ainda há NaNs\n",
    "        if df[feature_cols + [target_col]].isnull().any().any():\n",
    "            print(f\"Aviso: Ainda existem NaNs para {ticker}. Removendo linhas afetadas...\")\n",
    "            df.dropna(inplace=True)\n",
    "\n",
    "        print(f\"Dimensões finais do DataFrame para {ticker}: {df.shape}\")\n",
    "        if df.empty:\n",
    "            print(f\"Erro: DataFrame ficou vazio para {ticker} após limpeza.\")\n",
    "            return None, None, None\n",
    "\n",
    "        return df, feature_cols, target_col\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro CRÍTICO ao carregar/processar {caminho_arquivo}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# --- Função para criar janelas multivariadas com diferenciação ---\n",
    "def criar_janelas_multivariadas_diff(features_array, target_array, janela, add_diff=True):\n",
    "    \"\"\"Cria janelas multivariadas com diferenciação para melhorar previsões\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    if add_diff and features_array.shape[1] > 0:\n",
    "        # Adicionar colunas de diferenciação para cada feature\n",
    "        diffs = np.diff(features_array, axis=0)\n",
    "        # Concatenar com zeros no início para manter dimensões\n",
    "        zeros_row = np.zeros((1, diffs.shape[1]))\n",
    "        diffs_padded = np.vstack([zeros_row, diffs])\n",
    "        \n",
    "        # Combinar features originais com suas diferenciações\n",
    "        features_array = np.concatenate([features_array, diffs_padded], axis=1)\n",
    "    \n",
    "    if len(features_array) <= janela:\n",
    "        print(f\"Aviso: dados insuficientes ({len(features_array)}) para janela ({janela}).\")\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    for i in range(len(features_array) - janela):\n",
    "        X.append(features_array[i:(i + janela), :])\n",
    "        y.append(target_array[i + janela])  # Target já é T+1\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# --- Modelo LSTM melhorado com Residual Connections ---\n",
    "def build_model_improved(input_shape, dropout_rate=0.2):\n",
    "    \"\"\"Constrói modelo LSTM com conexões residuais para evitar previsões travadas\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Camada LSTM 1\n",
    "    lstm1 = layers.LSTM(100, return_sequences=True)(inputs)\n",
    "    lstm1 = layers.Dropout(dropout_rate)(lstm1)\n",
    "    \n",
    "    # Camada LSTM 2 com conexão residual\n",
    "    lstm2 = layers.LSTM(100, return_sequences=False)(lstm1)\n",
    "    lstm2 = layers.Dropout(dropout_rate)(lstm2)\n",
    "    \n",
    "    # Atenção à última dimensão da entrada para criar conexão residual\n",
    "    input_flattened = layers.Flatten()(inputs)\n",
    "    input_dense = layers.Dense(100)(input_flattened)\n",
    "    \n",
    "    # Combinar saída do LSTM e entrada (conexão residual)\n",
    "    combined = layers.Add()([lstm2, input_dense])\n",
    "    combined = layers.Activation('relu')(combined)\n",
    "    \n",
    "    # Camadas densas finais\n",
    "    dense1 = layers.Dense(64, activation='relu')(combined)\n",
    "    dense1 = layers.Dropout(dropout_rate/2)(dense1)\n",
    "    \n",
    "    # Saída\n",
    "    outputs = layers.Dense(1)(dense1)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- Função principal para treinar e avaliar o modelo ---\n",
    "def treinar_avaliar_modelo(ticker, janela, df_full, feature_cols, target_col, \n",
    "                           resultados_dir=\"resultados_lstm_macromicro_melhorado\"):\n",
    "    \"\"\"Treina e avalia o modelo LSTM melhorado para um ticker e janela\"\"\"\n",
    "    print(f\"\\n--- Iniciando Experimento: {ticker}, Janela {janela} ---\")\n",
    "    \n",
    "    os.makedirs(resultados_dir, exist_ok=True)\n",
    "    base_filename = f\"{resultados_dir}/{ticker}_Janela_{janela}\"\n",
    "    metrics_path = f\"{base_filename}_metrics.csv\"\n",
    "    grafico_path = f\"{base_filename}_grafico_teste_final.png\"\n",
    "    previsoes_path = f\"{base_filename}_previsoes_teste_final.csv\"\n",
    "    \n",
    "    if os.path.exists(metrics_path):\n",
    "        print(f\"Resultados já existem para {ticker}, Janela {janela}. Pulando...\")\n",
    "        return\n",
    "    \n",
    "    # 1. Separar Dados Treino/Validação (2020-2022) e Teste (2023)\n",
    "    start_date_val = \"2020-01-01\"\n",
    "    end_date_val = \"2022-12-31\"\n",
    "    start_date_test = \"2023-01-01\"\n",
    "    end_date_test = \"2023-12-31\"\n",
    "    \n",
    "    try:\n",
    "        df_val_train = df_full.loc[start_date_val:end_date_val].copy()\n",
    "        df_test_final = df_full.loc[start_date_test:end_date_test].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Erro ao dividir dados por data para {ticker}: {e}.\")\n",
    "        if not df_full.empty: \n",
    "            print(f\"Datas disponíveis: {df_full.index.min()} a {df_full.index.max()}\")\n",
    "        return\n",
    "    \n",
    "    if df_val_train.empty or df_test_final.empty:\n",
    "        print(f\"Erro: Período treino/val ou teste vazio para {ticker}.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Preparar dados\n",
    "    features_val_train = df_val_train[feature_cols].values\n",
    "    target_val_train = df_val_train[target_col].values\n",
    "    \n",
    "    features_test = df_test_final[feature_cols].values\n",
    "    target_test = df_test_final[target_col].values\n",
    "    \n",
    "    # 3. Escalonamento com StandardScaler (em vez de MinMaxScaler)\n",
    "    scaler_features = StandardScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "    \n",
    "    # Ajustar escaladores apenas nos dados de treino\n",
    "    scaled_features_val_train = scaler_features.fit_transform(features_val_train)\n",
    "    target_val_train_reshaped = target_val_train.reshape(-1, 1)\n",
    "    scaled_target_val_train = scaler_target.fit_transform(target_val_train_reshaped).flatten()\n",
    "    \n",
    "    # Transformar dados de teste\n",
    "    scaled_features_test = scaler_features.transform(features_test)\n",
    "    target_test_reshaped = target_test.reshape(-1, 1)\n",
    "    scaled_target_test = scaler_target.transform(target_test_reshaped).flatten()\n",
    "    \n",
    "    # 4. Criar janelas com diferenciação\n",
    "    X_train, y_train = criar_janelas_multivariadas_diff(\n",
    "        scaled_features_val_train, \n",
    "        scaled_target_val_train, \n",
    "        janela,\n",
    "        add_diff=True  # Adicionar colunas de diferenciação\n",
    "    )\n",
    "    \n",
    "    X_test, y_test = criar_janelas_multivariadas_diff(\n",
    "        scaled_features_test, \n",
    "        scaled_target_test, \n",
    "        janela,\n",
    "        add_diff=True  # Adicionar colunas de diferenciação\n",
    "    )\n",
    "    \n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(f\"Erro: Não foi possível criar janelas para {ticker}.\")\n",
    "        return\n",
    "    \n",
    "    # 5. Configurar validação\n",
    "    val_split_idx = int(0.8 * len(X_train))\n",
    "    X_train_final, y_train_final = X_train[:val_split_idx], y_train[:val_split_idx]\n",
    "    X_val, y_val = X_train[val_split_idx:], y_train[val_split_idx:]\n",
    "    \n",
    "    # 6. Construir e treinar modelo\n",
    "    input_shape = (janela, X_train.shape[2])\n",
    "    model = build_model_improved(input_shape)\n",
    "    \n",
    "    # Callbacks para melhorar treinamento\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=20, \n",
    "        restore_best_weights=True, \n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Treinar modelo\n",
    "    history = model.fit(\n",
    "        X_train_final, y_train_final,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 7. Avaliar no conjunto de teste\n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    predictions = scaler_target.inverse_transform(predictions_scaled).flatten()\n",
    "    actual = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 8. Calcular métricas\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actual, predictions)\n",
    "    \n",
    "    # 9. Verificar variação das previsões (para detectar previsões travadas)\n",
    "    var_real = np.std(actual) / np.mean(actual) if np.mean(actual) != 0 else 0\n",
    "    var_pred = np.std(predictions) / np.mean(predictions) if np.mean(predictions) != 0 else 0\n",
    "    prop_var = var_pred / var_real if var_real != 0 else 0\n",
    "    travado = \"Sim\" if prop_var < 0.3 else \"Não\"\n",
    "    \n",
    "    print(\"\\nResultados da Avaliação:\")\n",
    "    print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    print(f\"Variação Real: {var_real:.4f}, Variação Prevista: {var_pred:.4f}\")\n",
    "    print(f\"Previsões travadas? {travado}\")\n",
    "    \n",
    "    # 10. Salvar resultados\n",
    "    # Métricas\n",
    "    metrics_df = pd.DataFrame([{\n",
    "        'Ticker': ticker,\n",
    "        'Janela': janela,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Var_Real': var_real,\n",
    "        'Var_Pred': var_pred,\n",
    "        'Prop_Var': prop_var,\n",
    "        'Travado': travado\n",
    "    }])\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    \n",
    "    # Datas para as previsões\n",
    "    datas_teste = df_test_final.index[janela:janela+len(actual)]\n",
    "    \n",
    "    if len(datas_teste) == len(actual):\n",
    "        # Salvar previsões\n",
    "        previsoes_df = pd.DataFrame({\n",
    "            'Data': datas_teste,\n",
    "            'Preço Real': actual,\n",
    "            'Preço Previsto': predictions\n",
    "        })\n",
    "        previsoes_df.to_csv(previsoes_path, index=False)\n",
    "        \n",
    "        # Gerar gráfico\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Real'], label='Real', color='blue', linewidth=1.5)\n",
    "        plt.plot(previsoes_df['Data'], previsoes_df['Preço Previsto'], label='Previsto', color='orange', linestyle='--', linewidth=1.5)\n",
    "        plt.title(f'Preços Reais vs Previstos - {ticker} (Janela {janela})')\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('Preço')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.4)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(grafico_path)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"--- Experimento concluído: {ticker}, Janela {janela} ---\")\n",
    "    return metrics_df\n",
    "\n",
    "# --- Função para rodar experimentos em todos os tickers ---\n",
    "def rodar_experimentos_macromicro_melhorados():\n",
    "    pasta_dados = r\"C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\dados_unificados\"\n",
    "    resultados_dir = r\"C:\\Users\\leona\\pyhtonscripts\\CodigoExperimentos\\ExperimentoFeatures\\resultados_lstm_macromicro_melhorado\"\n",
    "    os.makedirs(resultados_dir, exist_ok=True)\n",
    "    \n",
    "    # Lista de tickers\n",
    "    tickers = [\"GGBR3.SA\"]\n",
    "    \n",
    "    # Janelas a testar\n",
    "    janelas = [1, 2, 3, 4, 5]\n",
    "    \n",
    "    resultados_consolidados = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        df_ticker_full, feature_cols, target_col = carregar_dados_ticker(ticker, pasta_dados)\n",
    "        \n",
    "        if df_ticker_full is None:\n",
    "            print(f\"Pulando ticker {ticker} devido a erro no carregamento.\")\n",
    "            continue\n",
    "        \n",
    "        for janela in janelas:\n",
    "            try:\n",
    "                metrics = treinar_avaliar_modelo(ticker, janela, df_ticker_full, feature_cols, target_col, resultados_dir)\n",
    "                if metrics is not None:\n",
    "                    resultados_consolidados.append(metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {ticker}, Janela {janela}: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Consolidar resultados\n",
    "    if resultados_consolidados:\n",
    "        df_resultados = pd.concat(resultados_consolidados)\n",
    "        df_resultados.to_csv(f\"{resultados_dir}/resultados_consolidados.csv\", index=False)\n",
    "        print(f\"\\nResultados consolidados salvos em {resultados_dir}/resultados_consolidados.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Desativar mensagens de aviso do TensorFlow\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    print(\"Iniciando experimentos com modelos melhorados para features macro/micro...\")\n",
    "    rodar_experimentos_macromicro_melhorados()\n",
    "    print(\"\\nTodos experimentos concluídos!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
